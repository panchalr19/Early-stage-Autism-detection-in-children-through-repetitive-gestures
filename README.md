# Early-stage-Autism-detection-in-children-through-repetitive-gestures
# Introduction
Early detection and diagnosis of autism spectrum disorders (ASDs) can increase children’s chances of benefiting from intervention while also easing the strain on concerned parents. ASD symptoms appear in the first two years of life, affecting multiple developmental domains and mapping onto symptom dimensions consistent with current diagnostic frameworks such as social communication and repetitive interests/behaviors, but also extending to motor delays and atypical regulation of attention and emotion, according to convergent data from both retrospective and prospective studies of high-risk infants. In infants eventually diagnosed with autism spectrum disorders, abnormalities in gesture use appear as early as 9–12 months of age, according to retrospective video investigations (ASD). Using a general population screening, we report on a prospective study of gesture use in children identified as at- risk for ASD. Infants who reached cutoffs for ”autism” on the ADOS at 20–24 months had more interrupted gestures than those who met cutoffs for ”autism spectrum” or those who did not meet cutoffs for either, despite the fact that these two groups had similar patterns of gesture use. As a result, infants that employ gestures may be more likely to benefit from early communication programs.


This file will Guide how to capture the data using the DataCollection.py file and train the model from FinalModel.py file.
### Prerequisites:
-	Python 
-	Tensorflow
-	A device in which the python environment is set and has a camera enabled or connected with.


## Getting Started:
Installation
1. Install the tensorflow library and import the other required libraries as mentioned in the files.
2. Importatnt libraries required
- os
-	cv2
-	numpy
-	matplotlib
-	mediapipe
-	sklearn
-	seaborn

### Execution of the code
1. Firstly, the DataCollection.py file is required to be executed. This will collect the data through your camera.
2. After data capturing is completed, the FinalModel.py file is required to be run.
3. To test the model, run the ‘Test in Real Time’ block. It will turn the camera on and will capture the person in front of the camera and give prediction. 


### Working AVI

https://user-images.githubusercontent.com/68776176/188217200-36a5dfb8-f525-4fe8-a511-9740dc88110a.mp4


